{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.pyplot as pyplot\n",
    "import PIL.Image as Image\n",
    "from PIL import ImageOps\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.cluster.vq import vq, kmeans, whiten\n",
    "import glob\n",
    "import math\n",
    "from sklearn.cluster import k_means\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[Larry's path] Brush_teeth_path = os.path.join(cur_dir,'data/Brush_teeth')\n",
    "#[Titus's path] r'C:/Users/titus/Documents/CS498/hw4/HMP_Dataset/\n",
    "cur_dir = os.path.dirname('__file__') #<-- absolute dir the script is in\n",
    "Brush_teeth_path = os.path.join(cur_dir,'data/Brush_teeth')\n",
    "Climb_stairs_MODEL_path = os.path.join(cur_dir,'data/Climb_stairs') \n",
    "Comb_hair_path = os.path.join(cur_dir,'data/Comb_hair')\n",
    "Descend_stairs_path = os.path.join(cur_dir,'data/Descend_stairs')\n",
    "Drink_glass_MODEL_path = os.path.join(cur_dir,'data/Drink_glass')\n",
    "Eat_meat_path = os.path.join(cur_dir,'data/Eat_meat') \n",
    "Eat_soup_path = os.path.join(cur_dir,'data/Eat_soup')\n",
    "Getup_bed_MODEL_path = os.path.join(cur_dir,'data/Getup_bed')\n",
    "liedown_bed_path = os.path.join(cur_dir,'data/liedown_bed')\n",
    "Pour_water_MODEL_path = os.path.join(cur_dir,'data/Pour_water')\n",
    "Sitdown_chair_MODEL_path = os.path.join(cur_dir,'data/Sitdown_chair')\n",
    "Standup_chair_MODEL_path = os.path.join(cur_dir,'data/Standup_chair')\n",
    "Use_telephone_path = os.path.join(cur_dir,'data/Use_telephone')\n",
    "Walk_MODEL_path = os.path.join(cur_dir,'data/Walk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Dataframes by Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_list = [Brush_teeth_path, Climb_stairs_MODEL_path, Comb_hair_path, Descend_stairs_path, Drink_glass_MODEL_path, Eat_meat_path, Eat_soup_path, Getup_bed_MODEL_path, liedown_bed_path, Pour_water_MODEL_path, Sitdown_chair_MODEL_path, Standup_chair_MODEL_path, Use_telephone_path, Walk_MODEL_path]\n",
    "df_list = []\n",
    "testDF_list = []\n",
    "test_df = []\n",
    "chunk_size = 32.0\n",
    "first_c = 20\n",
    "second_c = 12\n",
    "seg_len = int(3 * chunk_size)\n",
    "for path in path_list:\n",
    "    allFiles = glob.glob(path + \"/*.txt\")\n",
    "    a = False\n",
    "    for file_ in allFiles: #take every sample of one action in one single df\n",
    "        if a == False:\n",
    "            frame = pd.read_csv(file_,sep=\" \",index_col=None,  header=None)\n",
    "            a = True\n",
    "        else:\n",
    "            df = pd.read_csv(file_,sep=\" \",index_col=None,  header=None)\n",
    "            frame = pd.concat([frame,df])\n",
    "    newdf = frame.iloc[0:32,:]\n",
    "    newnp = newdf.as_matrix()\n",
    "    vq_df = pd.DataFrame(np.array(newnp).resize((1, seg_len)))\n",
    "    \n",
    "    #choseeing 20% to be the test data\n",
    "    test_threshold = math.floor(len(frame.index)/chunk_size)\n",
    "\n",
    "    set_vqDF = False\n",
    "    \n",
    "    for i in range (1, math.floor(len(frame.index)/chunk_size)):\n",
    "        newdf = frame.iloc[32*i:32*(i)+31, ]\n",
    "        newnp = newdf.as_matrix()\n",
    "        npreshape = pd.DataFrame(np.resize(newnp,(1, seg_len)))\n",
    "        if i < test_threshold:\n",
    "            vq_df = pd.concat([vq_df,npreshape])\n",
    "        else:\n",
    "            if set_vqDF == False: \n",
    "                set_vqDF = True \n",
    "                test_df = pd.DataFrame(np.array(newnp).resize((1,seg_len)))\n",
    "            else: \n",
    "                test_df = pd.concat([test_df,npreshape])\n",
    "    df_list.append(vq_df)\n",
    "    testDF_list.append(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Dataframes into one Train Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df_list[0]\n",
    "for df in range(1,len(df_list)):\n",
    "    train_df = pd.concat([train_df,df_list[df]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_list = [\"Brush_teeth_path\", \"Climb_stairs_MODEL_path\", \"Comb_hair_path\", \"Descend_stairs_path\", \"Drink_glass_MODEL_path\", \"Eat_meat_path\", \"Eat_soup_path\", \"Getup_bed_MODEL_path\", \"liedown_bed_path\", \"Pour_water_MODEL_path\", \"Sitdown_chair_MODEL_path\", \"Standup_chair_MODEL_path\", \"Use_telephone_path\", \"Walk_MODEL_path\"]\n",
    "#890\n",
    "init = True \n",
    "init_first_layer = [True]*first_c\n",
    "first_layer = [pd.DataFrame(np.array(newnp).resize((1,seg_len)))]*first_c\n",
    "whitene = whiten(train_df)\n",
    "centroid, label, inertia = k_means(whitene.astype(float), first_c)\n",
    "cent_from_first_layer = centroid\n",
    "cent_from_second_layer = [0]*first_c\n",
    "for i in range(label.shape[0]): \n",
    "    #row1 = (whitene.iloc[i]).as_matrix()\n",
    "    row2 = pd.DataFrame(whitene[i].reshape((1,seg_len)))\n",
    "    c = label[i]\n",
    "    if init_first_layer[c] == True:\n",
    "        init_first_layer[c] = False\n",
    "        first_layer[c] =row2\n",
    "    else:\n",
    "        first_layer[c] = pd.concat([first_layer[c],row2])\n",
    "histogram_vector = []\n",
    "for i in range(first_c):\n",
    "    cent_from_second_layer[i] = [0]*seg_len\n",
    "    cur_cluster = first_layer[i] \n",
    "    whitene = whiten(cur_cluster)\n",
    "    centroid, label, inertia = k_means(whitene.astype(float), second_c)\n",
    "    for j in range(second_c): \n",
    "        cent_from_second_layer[i][j] = centroid[j]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breaking txt files into chunks ; spliting files into training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "path_list = [Brush_teeth_path, Climb_stairs_MODEL_path, Comb_hair_path, Descend_stairs_path, Drink_glass_MODEL_path, Eat_meat_path, Eat_soup_path, Getup_bed_MODEL_path, liedown_bed_path, Pour_water_MODEL_path, Sitdown_chair_MODEL_path, Standup_chair_MODEL_path, Use_telephone_path, Walk_MODEL_path]\n",
    "train_y = []\n",
    "test_y = [] \n",
    "#path_list = [Brush_teeth_path] \n",
    "label = 0 \n",
    "init_histogram = False\n",
    "init_histogram2 = False\n",
    "\n",
    "\n",
    "for path in path_list:\n",
    "    allFiles = glob.glob(path + \"/*.txt\")\n",
    "    threshold = int(len(allFiles)*0.8)\n",
    "    trainFiles = allFiles[0:threshold]\n",
    "    testFiles = allFiles[threshold:len(allFiles)]\n",
    "    ###\n",
    "    # Training data\n",
    "    ###\n",
    "    for file_ in trainFiles: #per file\n",
    "        # make histogram per file \n",
    "        # find closest centroid\n",
    "        frame = pd.read_csv(file_,sep=\" \",index_col=None,  header=None)\n",
    "        newdf = frame.iloc[0:32,:]\n",
    "        newnp = newdf.as_matrix()\n",
    "        vq_df = pd.DataFrame(np.array(newnp).resize((1,seg_len)))\n",
    "        histogram = [0]*first_c*second_c\n",
    "        for i in range (0, math.floor(len(frame.index)/chunk_size)):\n",
    "            newdf = frame.iloc[32*i:32*(i)+31, ]\n",
    "            newnp = newdf.as_matrix()\n",
    "            npreshape = pd.DataFrame(np.resize(newnp,(1,seg_len)))\n",
    "            ####\n",
    "            # Find closest centroid \n",
    "            ####\n",
    "            min_val = 99999\n",
    "            min_cluster = 0\n",
    "            for j in range(len(cent_from_first_layer)):\n",
    "                val = np.sum(np.abs(cent_from_first_layer[j] - npreshape))\n",
    "                val = sum(val)\n",
    "                if val < min_val: \n",
    "                    min_val = val\n",
    "                    min_cluster = j\n",
    "            min_val2 = 99999\n",
    "            min_cluster2 = 0\n",
    "            for k in range(second_c):\n",
    "                val = np.sum(np.abs(cent_from_second_layer[min_cluster][k] - npreshape))\n",
    "                val = sum(val)\n",
    "                if val < min_val2: \n",
    "                    min_val2 = val\n",
    "                    min_cluster2 = k          \n",
    "            histogram[second_c*min_cluster + min_cluster2] = histogram[second_c*min_cluster + min_cluster2] + 1 \n",
    "        histogram = pd.DataFrame(np.array(histogram).reshape((1,first_c * second_c)))\n",
    "        train_y.append(label)\n",
    "        if init_histogram == False:\n",
    "            total_train = histogram\n",
    "            init_histogram = True\n",
    "        else:\n",
    "            total_train = pd.concat([total_train,histogram])\n",
    "    \n",
    "    ###\n",
    "    # Testing data\n",
    "    ###\n",
    "    for file_ in testFiles: #per file\n",
    "        # make histogram per file \n",
    "        # find closest centroid\n",
    "        frame = pd.read_csv(file_,sep=\" \",index_col=None,  header=None)\n",
    "        newdf = frame.iloc[0:32,:]\n",
    "        newnp = newdf.as_matrix()\n",
    "        vq_df = pd.DataFrame(np.array(newnp).resize((1,seg_len)))\n",
    "        histogram = [0]*first_c *second_c\n",
    "        for i in range (0, math.floor(len(frame.index)/chunk_size)):\n",
    "            newdf = frame.iloc[32*i:32*(i)+31, ]\n",
    "            newnp = newdf.as_matrix()\n",
    "            npreshape = pd.DataFrame(np.resize(newnp,(1,seg_len)))\n",
    "            ####\n",
    "            # Find closest centroid \n",
    "            ####\n",
    "            min_val = 99999\n",
    "            min_cluster = 0\n",
    "            for j in range(len(cent_from_first_layer)):\n",
    "                val = np.sum(np.abs(cent_from_first_layer[j] - npreshape))\n",
    "                val = sum(val)\n",
    "                if val < min_val: \n",
    "                    min_val = val\n",
    "                    min_cluster = j\n",
    "            min_val2 = 99999\n",
    "            min_cluster2 = 0\n",
    "            for k in range(12):\n",
    "                val = np.sum(np.abs(cent_from_second_layer[min_cluster][k] - npreshape))\n",
    "                val = sum(val)\n",
    "                if val < min_val2: \n",
    "                    min_val2 = val\n",
    "                    min_cluster2 = k          \n",
    "            histogram[second_c *min_cluster + min_cluster2] = histogram[second_c*min_cluster + min_cluster2] + 1   \n",
    "        histogram = pd.DataFrame(np.array(histogram).reshape((1,second_c * first_c)))\n",
    "        test_y.append(label)\n",
    "        if init_histogram2 == False:\n",
    "            total_test = histogram\n",
    "            init_histogram2 = True\n",
    "        else:\n",
    "            total_test = pd.concat([total_test,histogram])\n",
    "            \n",
    "    ###\n",
    "    # Update label \n",
    "    ###\n",
    "    label = label + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Accuracy and P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6069364161849711\n",
      "[[ 2  0  0  0  0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0 16  0  3  0  0  0  0  0  0  0  1  0  1]\n",
      " [ 0  0  4  0  1  0  0  2  0  0  0  0  0  0]\n",
      " [ 0  2  0  7  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  2  0 12  0  0  2  0  3  1  0  0  0]\n",
      " [ 0  0  0  0  0  1  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  1  0  0  0  0  0  0  0]\n",
      " [ 0  0  2  0  2  0  0 11  1  1  0  4  0  0]\n",
      " [ 0  0  0  0  1  0  0  1  1  1  0  2  0  0]\n",
      " [ 0  0  0  0  8  0  0  2  0  9  1  0  0  0]\n",
      " [ 0  0  1  0  1  0  0  2  0  0 11  5  0  0]\n",
      " [ 0  0  0  0  4  0  0  1  0  0  7  9  0  0]\n",
      " [ 0  0  1  0  1  0  0  0  0  0  0  0  1  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0 20]]\n",
      "0.7102102102102102\n",
      "[[ 9  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 75  0  4  0  0  0  2  0  0  0  0  0  0]\n",
      " [ 0  0 21  0  2  0  0  1  0  0  0  0  0  0]\n",
      " [ 0  1  0 32  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  4  0 48  0  0  6  0 20  1  0  1  0]\n",
      " [ 0  0  0  0  0  4  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  2  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  6  0  0 60  1  6  0  6  1  0]\n",
      " [ 0  0  1  0  3  0  0  6  4  2  0  6  0  0]\n",
      " [ 0  0  2  0 21  0  0 11  0 45  0  1  0  0]\n",
      " [ 0  0  0  0 12  0  0  5  0  6 49  8  0  0]\n",
      " [ 0  1  0  0 10  0  0  4  0  0 24 42  0  0]\n",
      " [ 0  0  0  0  1  0  1  0  0  0  0  0  8  0]\n",
      " [ 0  2  0  0  0  0  0  2  0  2  0  0  0 74]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=15,max_depth=15 )\n",
    "y_pred = rf.fit(total_train, train_y).predict(total_test)\n",
    "result = np.sum(np.in1d(test_y,y_pred))/len(y_pred)\n",
    "#test data \n",
    "print(sum([ x == y for x,y in zip(test_y, y_pred)])/len(y_pred))\n",
    "print(confusion_matrix(test_y, y_pred))\n",
    "#train data\n",
    "y_pred = rf.fit(total_train, train_y).predict(total_train)\n",
    "result = np.sum(np.in1d(train_y,y_pred))/len(y_pred)\n",
    "print(sum([ x == y for x,y in zip(train_y, y_pred)])/len(y_pred))\n",
    "print(confusion_matrix(train_y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
